{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ✨\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/80768ab97406b78de5e38b8c5835c412243425e9/complete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una pequeña aplicación encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalar dependencias"
      ],
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "id": "R6aBAC3pYLOK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la máquina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de código abierto que permite ejecutar modelos de lenguaje de gran tamaño (LLM) directamente en sus computadoras, sin necesidad de conexión a Internet ni servicios en la nube. Esto la convierte en una opción ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para más información pueden visitar [Ollama](https://ollama.com/)."
      ],
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "id": "xKIhyOf8XSUO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ],
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "id": "cOOT3FxzYARq"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e1bdd9b1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e1bdd9b1",
        "outputId": "fa1f0c8a-e6a1-4554-cf0b-c64e5a1c52a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 22.2 MB in 6s (3,759 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (610 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicación en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para más información pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para más información pueden visitar [Gradio](https://www.gradio.app/)."
      ],
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "id": "UuWq73LFYIoa"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "08d87df4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "08d87df4",
        "outputId": "8a2c8dc8-c36f-44f4-cdec-471c603ad50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install langchain_community gradio ollama pypdf --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la máquina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la máquina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ],
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "id": "HzYYFcBVZ_A3"
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "id": "cYzgfe_HaOpp",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la máquina local."
      ],
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "id": "Py2n2IDaa_R1"
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ],
      "metadata": {
        "id": "ktxYocQ0bHvD",
        "outputId": "0ebe8511-90ae-4724-82a3-30ce2cea8751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ktxYocQ0bHvD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProgressResponse(status='success', completed=None, total=None, digest=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ],
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "id": "1eWRyDY0bWV7"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ],
      "metadata": {
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yZ7rRKmZbVsA",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue because of a phenomenon called scattering, which is the way that light interacts with the tiny molecules of gases in the atmosphere.\n",
            "\n",
            "Here's what happens:\n",
            "\n",
            "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it consists of a range of colors, including all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
            "2. **Light is scattered by air molecules**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
            "3. **Shorter wavelengths are scattered more**: The smaller, shorter-wavelength blue light is scattered more than the longer-wavelength red light by these air molecules. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it.\n",
            "4. **Blue light reaches our eyes**: As a result of this scattering, the blue light is distributed throughout the sky, giving it a blue appearance when we look up at it.\n",
            "\n",
            "So, to summarize: the sky appears blue because of the way that sunlight interacts with the tiny molecules in the atmosphere, causing shorter-wavelength blue light to be scattered more than other colors.\n",
            "\n",
            "It's worth noting that this phenomenon occurs during the daytime when the sun is overhead. At sunrise and sunset, the sky can take on hues of red and orange due to a different effect called Mie scattering, which scatters longer wavelengths of light (like red and orange) in preference over shorter wavelengths (like blue).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podríamos usar la función \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar más el modelo y el proovedor."
      ],
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "id": "HblGBKi-cg3O"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(query)\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "R2KT4RIrccFD",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is the meaning of life?\n",
            "Assistant: The question of the meaning of life is one of the most profound and debated questions in human history. Philosophers, theologians, scientists, and everyday people have grappled with this query for centuries, and there's no single answer that satisfies everyone.\n",
            "\n",
            "That being said, here are some perspectives on what could be considered a possible answer to the question \"What is the meaning of life?\"\n",
            "\n",
            "**Existentialist View:**\n",
            "From an existentialist perspective, life has no inherent meaning. It's up to each individual to create their own purpose and meaning through their choices, actions, and experiences.\n",
            "\n",
            "**Philosophical Views:**\n",
            "\n",
            "1. **Stoicism:** The Stoics believed that the key to a meaningful life is living in accordance with reason and virtue. This involves aligning one's actions with universal principles of justice, morality, and self-control.\n",
            "2. **Utilitarianism:** Jeremy Bentham and John Stuart Mill proposed that the meaning of life lies in maximizing overall happiness or pleasure for oneself and others.\n",
            "3. **Hedonism:** Epicureanism suggests that the goal of life is to attain happiness through the pursuit of pleasure and the avoidance of pain.\n",
            "\n",
            "**Spiritual and Theological Views:**\n",
            "\n",
            "1. **Monotheistic Religions (e.g., Christianity, Islam, Judaism):** These traditions often believe that the purpose of life is to serve and follow God's will.\n",
            "2. **Eastern Philosophies (e.g., Buddhism, Taoism):** Many Eastern spiritualities suggest that the meaning of life lies in achieving a state of enlightenment or inner peace through self-reflection, meditation, and alignment with universal principles.\n",
            "\n",
            "**Scientific Perspectives:**\n",
            "\n",
            "1. **Biological View:** From a purely biological perspective, the purpose of life might be to survive, reproduce, and propagate one's genes.\n",
            "2. **Cosmological Perspective:** Some scientists argue that life has no inherent meaning but is simply a byproduct of the universe's evolution.\n",
            "\n",
            "**Personal and Subjective Views:**\n",
            "\n",
            "1. **Authenticity:** Many people believe that the meaning of life lies in being true to oneself, embracing individuality, and pursuing one's passions.\n",
            "2. **Relationships:** Others argue that meaningful relationships with family, friends, and community are essential for a fulfilling life.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal question that can't be reduced to a single answer or definition. It may vary from person to person, culture to culture, and context to context. What matters most is that each individual finds their own purpose and significance in life, even if it's not shared by others.\n",
            "\n",
            "What do you think the meaning of life is?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes “tal cual” se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Quién está hablando (user, assistant, system)\n",
        "- Cuál es el contexto\n",
        "- Dónde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas varían según el modelo y pueden incluir delimitadores especiales, instrucciones o formato específico. Generalmente se introducen tokens especiales antes de cada interacción. Esto le permite al modelo entender el contexto en el que interactúa.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA útil.\n",
        "<|user|>\n",
        "¿Cuál es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ],
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "id": "J5YT2nFAeHJp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mJLg-pNVfV5Y",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Assistant: Good morning, sir! Ah, a fine day indeed. I've taken the liberty of having your coffee and newspaper ready for you in the study. Shall I pour you a cup, sir?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione explícitamente el historial.\n",
        "- Cada interacción con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la “memoria” del modelo está limitada al contenido del mensaje que se le envía en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversación con un modelo como LLaMA, GPT o Mistral, el modelo no guarda información sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurrió en la ronda 1, solo podrá responder correctamente si se le incluye el historial de forma explícita."
      ],
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "id": "Qo8y0RgOgbV9"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "On_wIWx3idxC",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Welcome home, Master John. I've taken the liberty of preparing your favorite tea and arranging for the newspaper to be brought in. It's good to see you're settling back into routine after a long day.\n",
            "\n",
            "And congratulations are in order, sir! You're now three decades young. I trust all is well with you? Shall I assist you with anything or would you like to retire to your study for a while?\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Master John, sir! *adjusts gloves* Ah, yes... your age. As your loyal butler, I have the privilege of knowing many details about your life, including your birthdate and current chronology.\n",
            "\n",
            "You were born on July 1st, 1946, which makes you currently... *pauses for a moment to collect his thoughts* ...76 years young, sir!\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para dar la ilusión de memoria en aplicaciones prácticas, se pasa todo el historial de conversación como parte del nuevo mensaje (prompt chaining)."
      ],
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "id": "V5n3zVdRi4b8"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response.content}\")\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LT4hPK7hjDX6",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Good day, sir. It's a pleasure to make your acquaintance. I see you've introduced yourself as being 30 years of age. May I pour you a cup of tea while we chat? We have a lovely Earl Grey that's just been steeped to perfection.\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Good day, sir! Indeed, I do recall our previous conversation. You are 30 years young, as you so proudly introduced yourself. Shall I pour you a refill on that Earl Grey tea while we continue our conversation?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Texto dinámico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho más bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ],
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "id": "SKPNErWSj0b6"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  formated_message = prompt.invoke({\n",
        "      \"user\": \"John\",\n",
        "      \"message\": query,\n",
        "      \"history\": history\n",
        "  })\n",
        "\n",
        "  response = \"\"\n",
        "  # Usamos model.stream en vez de model.invoke\n",
        "  for chunk in model.stream(formated_message):\n",
        "    response += chunk.content\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "  print(\"\")\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response}\")"
      ],
      "metadata": {
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d8yNOHL8jrm9",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Good day, sir. How may I assist you today?\n",
            "User: Can you read today's schedule?\n",
            "Good question, sir! Let me just check the daily planner for you. (pause) Ah yes, here is your schedule for today:\n",
            "\n",
            "10:00 AM - Meeting with Mrs. Smith regarding estate matters\n",
            "12:30 PM - Lunch in the dining room\n",
            "2:00 PM - Tennis match at the country club\n",
            "4:00 PM - Meeting with your financial advisor to review quarterly reports\n",
            "\n",
            "Shall I make any adjustments or would you like me to prepare for the meetings, sir?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Aplicación con Gradio"
      ],
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "id": "pvExFNx2lzFE"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDuGfbeHlroE"
      },
      "id": "ZDuGfbeHlroE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}