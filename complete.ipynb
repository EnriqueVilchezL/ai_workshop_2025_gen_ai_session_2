{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ✨\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/complete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una pequeña aplicación encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalar dependencias"
      ],
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "id": "R6aBAC3pYLOK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la máquina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de código abierto que permite ejecutar modelos de lenguaje de gran tamaño (LLM) directamente en sus computadoras, sin necesidad de conexión a Internet ni servicios en la nube. Esto la convierte en una opción ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para más información pueden visitar [Ollama](https://ollama.com/)."
      ],
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "id": "xKIhyOf8XSUO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ],
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "id": "cOOT3FxzYARq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bdd9b1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bdd9b1",
        "outputId": "028b917e-ac0b-4f7f-885a-299092ca72dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 1s (218 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicación en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para más información pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para más información pueden visitar [Gradio](https://www.gradio.app/)."
      ],
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "id": "UuWq73LFYIoa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d87df4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d87df4",
        "outputId": "7073f8d6-141b-4892-8b21-f17410807b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain_community gradio ollama pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la máquina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la máquina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ],
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "id": "HzYYFcBVZ_A3"
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "id": "cYzgfe_HaOpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la máquina local."
      ],
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "id": "Py2n2IDaa_R1"
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxYocQ0bHvD",
        "outputId": "0ebe8511-90ae-4724-82a3-30ce2cea8751"
      },
      "id": "ktxYocQ0bHvD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProgressResponse(status='success', completed=None, total=None, digest=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ],
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "id": "1eWRyDY0bWV7"
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe"
      },
      "id": "yZ7rRKmZbVsA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue because of a phenomenon called scattering, which is the way that light interacts with the tiny molecules of gases in the atmosphere.\n",
            "\n",
            "Here's what happens:\n",
            "\n",
            "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it consists of a range of colors, including all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
            "2. **Light is scattered by air molecules**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
            "3. **Shorter wavelengths are scattered more**: The smaller, shorter-wavelength blue light is scattered more than the longer-wavelength red light by these air molecules. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it.\n",
            "4. **Blue light reaches our eyes**: As a result of this scattering, the blue light is distributed throughout the sky, giving it a blue appearance when we look up at it.\n",
            "\n",
            "So, to summarize: the sky appears blue because of the way that sunlight interacts with the tiny molecules in the atmosphere, causing shorter-wavelength blue light to be scattered more than other colors.\n",
            "\n",
            "It's worth noting that this phenomenon occurs during the daytime when the sun is overhead. At sunrise and sunset, the sky can take on hues of red and orange due to a different effect called Mie scattering, which scatters longer wavelengths of light (like red and orange) in preference over shorter wavelengths (like blue).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podríamos usar la función \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar más el modelo y el proovedor."
      ],
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "id": "HblGBKi-cg3O"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(query)\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca"
      },
      "id": "R2KT4RIrccFD",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is the meaning of life?\n",
            "Assistant: The question of the meaning of life is one of the most profound and debated questions in human history. Philosophers, theologians, scientists, and everyday people have grappled with this query for centuries, and there's no single answer that satisfies everyone.\n",
            "\n",
            "That being said, here are some perspectives on what could be considered a possible answer to the question \"What is the meaning of life?\"\n",
            "\n",
            "**Existentialist View:**\n",
            "From an existentialist perspective, life has no inherent meaning. It's up to each individual to create their own purpose and meaning through their choices, actions, and experiences.\n",
            "\n",
            "**Philosophical Views:**\n",
            "\n",
            "1. **Stoicism:** The Stoics believed that the key to a meaningful life is living in accordance with reason and virtue. This involves aligning one's actions with universal principles of justice, morality, and self-control.\n",
            "2. **Utilitarianism:** Jeremy Bentham and John Stuart Mill proposed that the meaning of life lies in maximizing overall happiness or pleasure for oneself and others.\n",
            "3. **Hedonism:** Epicureanism suggests that the goal of life is to attain happiness through the pursuit of pleasure and the avoidance of pain.\n",
            "\n",
            "**Spiritual and Theological Views:**\n",
            "\n",
            "1. **Monotheistic Religions (e.g., Christianity, Islam, Judaism):** These traditions often believe that the purpose of life is to serve and follow God's will.\n",
            "2. **Eastern Philosophies (e.g., Buddhism, Taoism):** Many Eastern spiritualities suggest that the meaning of life lies in achieving a state of enlightenment or inner peace through self-reflection, meditation, and alignment with universal principles.\n",
            "\n",
            "**Scientific Perspectives:**\n",
            "\n",
            "1. **Biological View:** From a purely biological perspective, the purpose of life might be to survive, reproduce, and propagate one's genes.\n",
            "2. **Cosmological Perspective:** Some scientists argue that life has no inherent meaning but is simply a byproduct of the universe's evolution.\n",
            "\n",
            "**Personal and Subjective Views:**\n",
            "\n",
            "1. **Authenticity:** Many people believe that the meaning of life lies in being true to oneself, embracing individuality, and pursuing one's passions.\n",
            "2. **Relationships:** Others argue that meaningful relationships with family, friends, and community are essential for a fulfilling life.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal question that can't be reduced to a single answer or definition. It may vary from person to person, culture to culture, and context to context. What matters most is that each individual finds their own purpose and significance in life, even if it's not shared by others.\n",
            "\n",
            "What do you think the meaning of life is?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes “tal cual” se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Quién está hablando (user, assistant, system)\n",
        "- Cuál es el contexto\n",
        "- Dónde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas varían según el modelo y pueden incluir delimitadores especiales, instrucciones o formato específico. Generalmente se introducen tokens especiales antes de cada interacción. Esto le permite al modelo entender el contexto en el que interactúa.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA útil.\n",
        "<|user|>\n",
        "¿Cuál es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ],
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "id": "J5YT2nFAeHJp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0"
      },
      "id": "mJLg-pNVfV5Y",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Assistant: Good morning, sir! Ah, a fine day indeed. I've taken the liberty of having your coffee and newspaper ready for you in the study. Shall I pour you a cup, sir?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione explícitamente el historial.\n",
        "- Cada interacción con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la “memoria” del modelo está limitada al contenido del mensaje que se le envía en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversación con un modelo como LLaMA, GPT o Mistral, el modelo no guarda información sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurrió en la ronda 1, solo podrá responder correctamente si se le incluye el historial de forma explícita."
      ],
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "id": "Qo8y0RgOgbV9"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056"
      },
      "id": "On_wIWx3idxC",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Welcome home, Master John. I've taken the liberty of preparing your favorite tea and arranging for the newspaper to be brought in. It's good to see you're settling back into routine after a long day.\n",
            "\n",
            "And congratulations are in order, sir! You're now three decades young. I trust all is well with you? Shall I assist you with anything or would you like to retire to your study for a while?\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Master John, sir! *adjusts gloves* Ah, yes... your age. As your loyal butler, I have the privilege of knowing many details about your life, including your birthdate and current chronology.\n",
            "\n",
            "You were born on July 1st, 1946, which makes you currently... *pauses for a moment to collect his thoughts* ...76 years young, sir!\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para dar la ilusión de memoria en aplicaciones prácticas, se pasa todo el historial de conversación como parte del nuevo mensaje (prompt chaining)."
      ],
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "id": "V5n3zVdRi4b8"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response.content}\")\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb"
      },
      "id": "LT4hPK7hjDX6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Good day, sir. It's a pleasure to make your acquaintance. I see you've introduced yourself as being 30 years of age. May I pour you a cup of tea while we chat? We have a lovely Earl Grey that's just been steeped to perfection.\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Good day, sir! Indeed, I do recall our previous conversation. You are 30 years young, as you so proudly introduced yourself. Shall I pour you a refill on that Earl Grey tea while we continue our conversation?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Texto dinámico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho más bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ],
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "id": "SKPNErWSj0b6"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  formated_message = prompt.invoke({\n",
        "      \"user\": \"John\",\n",
        "      \"message\": query,\n",
        "      \"history\": history\n",
        "  })\n",
        "\n",
        "  response = \"\"\n",
        "  # Usamos model.stream en vez de model.invoke\n",
        "  for chunk in model.stream(formated_message):\n",
        "    response += chunk.content\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "  print(\"\")\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd"
      },
      "id": "d8yNOHL8jrm9",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Good day, sir. How may I assist you today?\n",
            "User: Can you read today's schedule?\n",
            "Good question, sir! Let me just check the daily planner for you. (pause) Ah yes, here is your schedule for today:\n",
            "\n",
            "10:00 AM - Meeting with Mrs. Smith regarding estate matters\n",
            "12:30 PM - Lunch in the dining room\n",
            "2:00 PM - Tennis match at the country club\n",
            "4:00 PM - Meeting with your financial advisor to review quarterly reports\n",
            "\n",
            "Shall I make any adjustments or would you like me to prepare for the meetings, sir?\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Aplicación con Gradio\n",
        "\n",
        "Lo que hemos hecho hasta ahora está muy bien, pero quizá queremos mostrarle nuestro trabajo a nuestros amigos... Gradio ofrece una interfaz muy sencilla para montar una aplicación web que sirva para LLMs."
      ],
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "id": "pvExFNx2lzFE"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "def chat(message: str, history: list[str]) -> str:\n",
        "  \"\"\"\n",
        "  Function to chat with the ollama model.\n",
        "  \"\"\"\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": message,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  return response.content\n",
        "\n",
        "demo = gr.ChatInterface(fn=chat)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "ZDuGfbeHlroE",
        "outputId": "b588fa0c-b0ee-44f4-f5fc-9e97fd2b96b2"
      },
      "id": "ZDuGfbeHlroE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://89bf4db6722e2075d5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89bf4db6722e2075d5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Análisis de archivos\n",
        "\n",
        "Una posible aplicación interesante de los LLMs es utilizarlos para analizar archivos como PDFs o TXTs. Con gradio es sumamente sencillo integrar esto a la interfaz web."
      ],
      "metadata": {
        "id": "V8EhpkzLq1G4"
      },
      "id": "V8EhpkzLq1G4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import gradio as gr\n",
        "\n",
        "# Load model\n",
        "model = init_chat_model(model=\"gemma3:12b\", model_provider=\"ollama\")\n",
        "\n",
        "# Prompt\n",
        "messages = (\"Human\",\n",
        "    \"\"\"\n",
        "    You are a helpful butler named Alfred, working for {user}.\n",
        "\n",
        "    Documents:\n",
        "    {documents}\n",
        "\n",
        "    Message history:\n",
        "    {history}\n",
        "\n",
        "    Last message:\n",
        "    {message}\n",
        "    \"\"\")\n",
        "\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "# Chat logic\n",
        "def chat_fn(message, history, session_documents):\n",
        "    response = model.invoke(prompt.invoke({\n",
        "        \"user\": \"John\",\n",
        "        \"history\": history,\n",
        "        \"message\": message,\n",
        "        \"documents\": session_documents\n",
        "    }))\n",
        "    return response.content\n",
        "\n",
        "# File loader\n",
        "def load_document(file, session_documents):\n",
        "    print(\"Loading:\", file.name)\n",
        "    if file.name.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file.name)\n",
        "    elif file.name.endswith(\".txt\"):\n",
        "        loader = TextLoader(file.name)\n",
        "    else:\n",
        "        return \"Unsupported file type\", session_documents\n",
        "\n",
        "    docs = loader.load()\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    updated_docs = session_documents + texts\n",
        "\n",
        "    print(\"Total documents now:\", len(updated_docs))\n",
        "    return \"Document loaded\", updated_docs\n",
        "\n",
        "# Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Alfred, Your Document Butler\")\n",
        "\n",
        "    # Per-session doc memory\n",
        "    session_documents = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(file_types=[\".pdf\", \".txt\"], label=\"Upload a document\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    file_input.change(fn=load_document, inputs=[file_input, session_documents], outputs=[status, session_documents])\n",
        "\n",
        "    chatbot = gr.ChatInterface(\n",
        "        fn=chat_fn,\n",
        "        additional_inputs=[session_documents]\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "iae3q2pRq0xD"
      },
      "id": "iae3q2pRq0xD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}