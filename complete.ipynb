{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ✨\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/complete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una pequeña aplicación encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6aBAC3pYLOK",
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "source": [
        "## 1. Instalar dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKIhyOf8XSUO",
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la máquina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de código abierto que permite ejecutar modelos de lenguaje de gran tamaño (LLM) directamente en sus computadoras, sin necesidad de conexión a Internet ni servicios en la nube. Esto la convierte en una opción ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para más información pueden visitar [Ollama](https://ollama.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cOOT3FxzYARq",
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e1bdd9b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bdd9b1",
        "outputId": "9e59b9e0-b1a3-443e-a747-155139dff597",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Fetched 22.2 MB in 7s (3,311 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 2s (201 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UuWq73LFYIoa",
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicación en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para más información pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para más información pueden visitar [Gradio](https://www.gradio.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "08d87df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d87df4",
        "outputId": "79bda925-bbfb-478e-e5a5-e7be6e00d4cf",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.31)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, groovy, ffmpy, aiofiles, typing-inspect, starlette, safehttpx, pydantic-settings, ollama, gradio-client, fastapi, dataclasses-json, gradio, langchain_community\n",
            "Successfully installed aiofiles-24.1.0 dataclasses-json-0.6.7 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 httpx-sse-0.4.0 langchain_community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 ollama-0.4.8 pydantic-settings-2.9.1 pydub-0.25.1 pypdf-5.4.0 python-dotenv-1.1.0 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 typing-inspect-0.9.0 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain_community gradio ollama pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la máquina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzYYFcBVZ_A3",
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la máquina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cYzgfe_HaOpp",
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Py2n2IDaa_R1",
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la máquina local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ktxYocQ0bHvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxYocQ0bHvD",
        "outputId": "b2632382-bd46-40ec-eb50-6a4419faf3f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ProgressResponse(status='success', completed=None, total=None, digest=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eWRyDY0bWV7",
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZ7rRKmZbVsA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sky appears blue because of a phenomenon called scattering, which is the way that light interacts with the tiny molecules of gases in the atmosphere.\n",
            "\n",
            "Here's what happens:\n",
            "\n",
            "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it consists of a range of colors, including all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
            "2. **Light is scattered by air molecules**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
            "3. **Shorter wavelengths are scattered more**: The smaller, shorter-wavelength blue light is scattered more than the longer-wavelength red light by these air molecules. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it.\n",
            "4. **Blue light reaches our eyes**: As a result of this scattering, the blue light is distributed throughout the sky, giving it a blue appearance when we look up at it.\n",
            "\n",
            "So, to summarize: the sky appears blue because of the way that sunlight interacts with the tiny molecules in the atmosphere, causing shorter-wavelength blue light to be scattered more than other colors.\n",
            "\n",
            "It's worth noting that this phenomenon occurs during the daytime when the sun is overhead. At sunrise and sunset, the sky can take on hues of red and orange due to a different effect called Mie scattering, which scatters longer wavelengths of light (like red and orange) in preference over shorter wavelengths (like blue).\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HblGBKi-cg3O",
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podríamos usar la función \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar más el modelo y el proovedor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R2KT4RIrccFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is the meaning of life?\n",
            "Assistant: The question of the meaning of life is one of the most profound and debated questions in human history. Philosophers, theologians, scientists, and everyday people have grappled with this query for centuries, and there's no single answer that satisfies everyone.\n",
            "\n",
            "That being said, here are some perspectives on what could be considered a possible answer to the question \"What is the meaning of life?\"\n",
            "\n",
            "**Existentialist View:**\n",
            "From an existentialist perspective, life has no inherent meaning. It's up to each individual to create their own purpose and meaning through their choices, actions, and experiences.\n",
            "\n",
            "**Philosophical Views:**\n",
            "\n",
            "1. **Stoicism:** The Stoics believed that the key to a meaningful life is living in accordance with reason and virtue. This involves aligning one's actions with universal principles of justice, morality, and self-control.\n",
            "2. **Utilitarianism:** Jeremy Bentham and John Stuart Mill proposed that the meaning of life lies in maximizing overall happiness or pleasure for oneself and others.\n",
            "3. **Hedonism:** Epicureanism suggests that the goal of life is to attain happiness through the pursuit of pleasure and the avoidance of pain.\n",
            "\n",
            "**Spiritual and Theological Views:**\n",
            "\n",
            "1. **Monotheistic Religions (e.g., Christianity, Islam, Judaism):** These traditions often believe that the purpose of life is to serve and follow God's will.\n",
            "2. **Eastern Philosophies (e.g., Buddhism, Taoism):** Many Eastern spiritualities suggest that the meaning of life lies in achieving a state of enlightenment or inner peace through self-reflection, meditation, and alignment with universal principles.\n",
            "\n",
            "**Scientific Perspectives:**\n",
            "\n",
            "1. **Biological View:** From a purely biological perspective, the purpose of life might be to survive, reproduce, and propagate one's genes.\n",
            "2. **Cosmological Perspective:** Some scientists argue that life has no inherent meaning but is simply a byproduct of the universe's evolution.\n",
            "\n",
            "**Personal and Subjective Views:**\n",
            "\n",
            "1. **Authenticity:** Many people believe that the meaning of life lies in being true to oneself, embracing individuality, and pursuing one's passions.\n",
            "2. **Relationships:** Others argue that meaningful relationships with family, friends, and community are essential for a fulfilling life.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal question that can't be reduced to a single answer or definition. It may vary from person to person, culture to culture, and context to context. What matters most is that each individual finds their own purpose and significance in life, even if it's not shared by others.\n",
            "\n",
            "What do you think the meaning of life is?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(query)\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J5YT2nFAeHJp",
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes “tal cual” se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Quién está hablando (user, assistant, system)\n",
        "- Cuál es el contexto\n",
        "- Dónde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas varían según el modelo y pueden incluir delimitadores especiales, instrucciones o formato específico. Generalmente se introducen tokens especiales antes de cada interacción. Esto le permite al modelo entender el contexto en el que interactúa.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA útil.\n",
        "<|user|>\n",
        "¿Cuál es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJLg-pNVfV5Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Assistant: Good morning, sir! Ah, a fine day indeed. I've taken the liberty of having your coffee and newspaper ready for you in the study. Shall I pour you a cup, sir?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qo8y0RgOgbV9",
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione explícitamente el historial.\n",
        "- Cada interacción con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la “memoria” del modelo está limitada al contenido del mensaje que se le envía en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversación con un modelo como LLaMA, GPT o Mistral, el modelo no guarda información sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurrió en la ronda 1, solo podrá responder correctamente si se le incluye el historial de forma explícita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "On_wIWx3idxC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Welcome home, Master John. I've taken the liberty of preparing your favorite tea and arranging for the newspaper to be brought in. It's good to see you're settling back into routine after a long day.\n",
            "\n",
            "And congratulations are in order, sir! You're now three decades young. I trust all is well with you? Shall I assist you with anything or would you like to retire to your study for a while?\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Master John, sir! *adjusts gloves* Ah, yes... your age. As your loyal butler, I have the privilege of knowing many details about your life, including your birthdate and current chronology.\n",
            "\n",
            "You were born on July 1st, 1946, which makes you currently... *pauses for a moment to collect his thoughts* ...76 years young, sir!\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V5n3zVdRi4b8",
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "source": [
        "Para dar la ilusión de memoria en aplicaciones prácticas, se pasa todo el historial de conversación como parte del nuevo mensaje (prompt chaining)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LT4hPK7hjDX6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Good day, sir. It's a pleasure to make your acquaintance. I see you've introduced yourself as being 30 years of age. May I pour you a cup of tea while we chat? We have a lovely Earl Grey that's just been steeped to perfection.\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Good day, sir! Indeed, I do recall our previous conversation. You are 30 years young, as you so proudly introduced yourself. Shall I pour you a refill on that Earl Grey tea while we continue our conversation?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response.content}\")\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SKPNErWSj0b6",
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "source": [
        "### 2.3. Texto dinámico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho más bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8yNOHL8jrm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Good day, sir. How may I assist you today?\n",
            "User: Can you read today's schedule?\n",
            "Good question, sir! Let me just check the daily planner for you. (pause) Ah yes, here is your schedule for today:\n",
            "\n",
            "10:00 AM - Meeting with Mrs. Smith regarding estate matters\n",
            "12:30 PM - Lunch in the dining room\n",
            "2:00 PM - Tennis match at the country club\n",
            "4:00 PM - Meeting with your financial advisor to review quarterly reports\n",
            "\n",
            "Shall I make any adjustments or would you like me to prepare for the meetings, sir?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  formated_message = prompt.invoke({\n",
        "      \"user\": \"John\",\n",
        "      \"message\": query,\n",
        "      \"history\": history\n",
        "  })\n",
        "\n",
        "  response = \"\"\n",
        "  # Usamos model.stream en vez de model.invoke\n",
        "  for chunk in model.stream(formated_message):\n",
        "    response += chunk.content\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "  print(\"\")\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvExFNx2lzFE",
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "source": [
        "## 3. Aplicación con Gradio\n",
        "\n",
        "Lo que hemos hecho hasta ahora está muy bien, pero quizá queremos mostrarle nuestro trabajo a nuestros amigos... Gradio ofrece una interfaz muy sencilla para montar una aplicación web que sirva para LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDuGfbeHlroE",
      "metadata": {
        "id": "ZDuGfbeHlroE"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "def chat(message: str, history: list[str]) -> str:\n",
        "  \"\"\"\n",
        "  Function to chat with the ollama model.\n",
        "  \"\"\"\n",
        "  formated_message = prompt.invoke({\n",
        "      \"user\": \"John\",\n",
        "      \"message\": message,\n",
        "      \"history\": history\n",
        "  })\n",
        "\n",
        "  response = \"\"\n",
        "  for chunk in model.stream(formated_message):\n",
        "    response += chunk.content\n",
        "    yield response\n",
        "\n",
        "demo = gr.ChatInterface(fn=chat).queue()\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8EhpkzLq1G4",
      "metadata": {
        "id": "V8EhpkzLq1G4"
      },
      "source": [
        "## 4. Análisis de archivos\n",
        "\n",
        "Una posible aplicación interesante de los LLMs es utilizarlos para analizar archivos como PDFs o TXTs. Con gradio es sumamente sencillo integrar esto a la interfaz web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iae3q2pRq0xD",
      "metadata": {
        "id": "iae3q2pRq0xD"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import gradio as gr\n",
        "\n",
        "# Load model\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Prompt\n",
        "messages = (\"Human\",\n",
        "    \"\"\"\n",
        "    You are a helpful butler named Alfred, working for {user}.\n",
        "\n",
        "    Documents:\n",
        "    {documents}\n",
        "\n",
        "    Message history:\n",
        "    {history}\n",
        "\n",
        "    Last message:\n",
        "    {message}\n",
        "    \"\"\")\n",
        "\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "# Chat logic\n",
        "def chat_fn(message, history, session_documents):\n",
        "    formated_message = prompt.invoke({\n",
        "        \"user\": \"John\",\n",
        "        \"history\": history,\n",
        "        \"message\": message,\n",
        "        \"documents\": session_documents\n",
        "    })\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in model.stream(formated_message):\n",
        "        response += chunk.content\n",
        "        yield response\n",
        "\n",
        "# File loader\n",
        "def load_document(file, session_documents):\n",
        "    print(\"Loading:\", file.name)\n",
        "    if file.name.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file.name)\n",
        "    elif file.name.endswith(\".txt\"):\n",
        "        loader = TextLoader(file.name)\n",
        "    else:\n",
        "        return \"Unsupported file type\", session_documents\n",
        "\n",
        "    docs = loader.load()\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    updated_docs = session_documents + texts\n",
        "\n",
        "    print(\"Total documents now:\", len(updated_docs))\n",
        "    return \"Document loaded\", updated_docs\n",
        "\n",
        "# Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Alfred, Your Document Butler\")\n",
        "\n",
        "    # Per-session doc memory\n",
        "    session_documents = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(file_types=[\".pdf\", \".txt\"], label=\"Upload a document\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    file_input.change(fn=load_document, inputs=[file_input, session_documents], outputs=[status, session_documents])\n",
        "\n",
        "    chatbot = gr.ChatInterface(\n",
        "        fn=chat_fn,\n",
        "        additional_inputs=[session_documents]\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0119a25",
      "metadata": {
        "id": "e0119a25"
      },
      "source": [
        "## 5. Conclusiones\n",
        "\n",
        "Este taller ha demostrado que es totalmente posible correr modelos de lenguaje grandes (LLMs) **de forma local**, sin depender de servicios externos en la nube. Gracias a herramientas como **Ollama**, **Langchain** y **Gradio**, puedes construir rápidamente aplicaciones conversacionales personalizadas, privadas y adaptables.\n",
        "\n",
        "Al finalizar este notebook, deberías tener un conocimiento básico sobre:\n",
        "\n",
        "- Cómo instalar y correr LLMs localmente usando Ollama.\n",
        "- Cómo crear interacciones personalizadas y memorias conversacionales con Langchain.\n",
        "- Cómo construir una interfaz de usuario sencilla con Gradio.\n",
        "- Cómo extender la funcionalidad del modelo para trabajar con archivos (como PDFs).\n",
        "\n",
        "Este es solo el comienzo: pueden adaptar esta arquitectura para hacer agentes inteligentes (ver [Curso de agentes](https://huggingface.co/learn/agents-course/en/unit0/introduction)), asistentes para tu trabajo, chatbots educativos, o incluso sistemas RAG (ver [Taller de RAG con uv](https://github.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session.git) y [Taller de RAG con poetry](https://github.com/Antonio-Tresol/ai_workshop_2025_gen_ai_session.git)).\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por participar en el taller 🙌 ¡A seguir creando con IA generativa!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}