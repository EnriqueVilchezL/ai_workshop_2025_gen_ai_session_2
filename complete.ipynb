{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ‚ú®\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/complete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una peque√±a aplicaci√≥n encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6aBAC3pYLOK",
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "source": [
        "## 1. Instalar dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKIhyOf8XSUO",
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la m√°quina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de c√≥digo abierto que permite ejecutar modelos de lenguaje de gran tama√±o (LLM) directamente en sus computadoras, sin necesidad de conexi√≥n a Internet ni servicios en la nube. Esto la convierte en una opci√≥n ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para m√°s informaci√≥n pueden visitar [Ollama](https://ollama.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cOOT3FxzYARq",
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bdd9b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bdd9b1",
        "outputId": "028b917e-ac0b-4f7f-885a-299092ca72dc",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 1s (218 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UuWq73LFYIoa",
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicaci√≥n en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para m√°s informaci√≥n pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para m√°s informaci√≥n pueden visitar [Gradio](https://www.gradio.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d87df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d87df4",
        "outputId": "7073f8d6-141b-4892-8b21-f17410807b1e",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.52)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain_community gradio ollama pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la m√°quina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzYYFcBVZ_A3",
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la m√°quina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cYzgfe_HaOpp",
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Py2n2IDaa_R1",
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la m√°quina local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ktxYocQ0bHvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxYocQ0bHvD",
        "outputId": "0ebe8511-90ae-4724-82a3-30ce2cea8751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ProgressResponse(status='success', completed=None, total=None, digest=None)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eWRyDY0bWV7",
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZ7rRKmZbVsA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sky appears blue because of a phenomenon called scattering, which is the way that light interacts with the tiny molecules of gases in the atmosphere.\n",
            "\n",
            "Here's what happens:\n",
            "\n",
            "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it consists of a range of colors, including all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
            "2. **Light is scattered by air molecules**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
            "3. **Shorter wavelengths are scattered more**: The smaller, shorter-wavelength blue light is scattered more than the longer-wavelength red light by these air molecules. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it.\n",
            "4. **Blue light reaches our eyes**: As a result of this scattering, the blue light is distributed throughout the sky, giving it a blue appearance when we look up at it.\n",
            "\n",
            "So, to summarize: the sky appears blue because of the way that sunlight interacts with the tiny molecules in the atmosphere, causing shorter-wavelength blue light to be scattered more than other colors.\n",
            "\n",
            "It's worth noting that this phenomenon occurs during the daytime when the sun is overhead. At sunrise and sunset, the sky can take on hues of red and orange due to a different effect called Mie scattering, which scatters longer wavelengths of light (like red and orange) in preference over shorter wavelengths (like blue).\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HblGBKi-cg3O",
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podr√≠amos usar la funci√≥n \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar m√°s el modelo y el proovedor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R2KT4RIrccFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: What is the meaning of life?\n",
            "Assistant: The question of the meaning of life is one of the most profound and debated questions in human history. Philosophers, theologians, scientists, and everyday people have grappled with this query for centuries, and there's no single answer that satisfies everyone.\n",
            "\n",
            "That being said, here are some perspectives on what could be considered a possible answer to the question \"What is the meaning of life?\"\n",
            "\n",
            "**Existentialist View:**\n",
            "From an existentialist perspective, life has no inherent meaning. It's up to each individual to create their own purpose and meaning through their choices, actions, and experiences.\n",
            "\n",
            "**Philosophical Views:**\n",
            "\n",
            "1. **Stoicism:** The Stoics believed that the key to a meaningful life is living in accordance with reason and virtue. This involves aligning one's actions with universal principles of justice, morality, and self-control.\n",
            "2. **Utilitarianism:** Jeremy Bentham and John Stuart Mill proposed that the meaning of life lies in maximizing overall happiness or pleasure for oneself and others.\n",
            "3. **Hedonism:** Epicureanism suggests that the goal of life is to attain happiness through the pursuit of pleasure and the avoidance of pain.\n",
            "\n",
            "**Spiritual and Theological Views:**\n",
            "\n",
            "1. **Monotheistic Religions (e.g., Christianity, Islam, Judaism):** These traditions often believe that the purpose of life is to serve and follow God's will.\n",
            "2. **Eastern Philosophies (e.g., Buddhism, Taoism):** Many Eastern spiritualities suggest that the meaning of life lies in achieving a state of enlightenment or inner peace through self-reflection, meditation, and alignment with universal principles.\n",
            "\n",
            "**Scientific Perspectives:**\n",
            "\n",
            "1. **Biological View:** From a purely biological perspective, the purpose of life might be to survive, reproduce, and propagate one's genes.\n",
            "2. **Cosmological Perspective:** Some scientists argue that life has no inherent meaning but is simply a byproduct of the universe's evolution.\n",
            "\n",
            "**Personal and Subjective Views:**\n",
            "\n",
            "1. **Authenticity:** Many people believe that the meaning of life lies in being true to oneself, embracing individuality, and pursuing one's passions.\n",
            "2. **Relationships:** Others argue that meaningful relationships with family, friends, and community are essential for a fulfilling life.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal question that can't be reduced to a single answer or definition. It may vary from person to person, culture to culture, and context to context. What matters most is that each individual finds their own purpose and significance in life, even if it's not shared by others.\n",
            "\n",
            "What do you think the meaning of life is?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(query)\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J5YT2nFAeHJp",
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes ‚Äútal cual‚Äù se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Qui√©n est√° hablando (user, assistant, system)\n",
        "- Cu√°l es el contexto\n",
        "- D√≥nde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas var√≠an seg√∫n el modelo y pueden incluir delimitadores especiales, instrucciones o formato espec√≠fico. Generalmente se introducen tokens especiales antes de cada interacci√≥n. Esto le permite al modelo entender el contexto en el que interact√∫a.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA √∫til.\n",
        "<|user|>\n",
        "¬øCu√°l es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJLg-pNVfV5Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Assistant: Good morning, sir! Ah, a fine day indeed. I've taken the liberty of having your coffee and newspaper ready for you in the study. Shall I pour you a cup, sir?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qo8y0RgOgbV9",
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione expl√≠citamente el historial.\n",
        "- Cada interacci√≥n con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la ‚Äúmemoria‚Äù del modelo est√° limitada al contenido del mensaje que se le env√≠a en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversaci√≥n con un modelo como LLaMA, GPT o Mistral, el modelo no guarda informaci√≥n sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurri√≥ en la ronda 1, solo podr√° responder correctamente si se le incluye el historial de forma expl√≠cita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "On_wIWx3idxC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Welcome home, Master John. I've taken the liberty of preparing your favorite tea and arranging for the newspaper to be brought in. It's good to see you're settling back into routine after a long day.\n",
            "\n",
            "And congratulations are in order, sir! You're now three decades young. I trust all is well with you? Shall I assist you with anything or would you like to retire to your study for a while?\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Master John, sir! *adjusts gloves* Ah, yes... your age. As your loyal butler, I have the privilege of knowing many details about your life, including your birthdate and current chronology.\n",
            "\n",
            "You were born on July 1st, 1946, which makes you currently... *pauses for a moment to collect his thoughts* ...76 years young, sir!\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query\n",
        "      })\n",
        "  )\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V5n3zVdRi4b8",
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "source": [
        "Para dar la ilusi√≥n de memoria en aplicaciones pr√°cticas, se pasa todo el historial de conversaci√≥n como parte del nuevo mensaje (prompt chaining)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LT4hPK7hjDX6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello! I am 30 years old\n",
            "Assistant: Good day, sir. It's a pleasure to make your acquaintance. I see you've introduced yourself as being 30 years of age. May I pour you a cup of tea while we chat? We have a lovely Earl Grey that's just been steeped to perfection.\n",
            "User: How old am I? Do you remember?\n",
            "Assistant: Good day, sir! Indeed, I do recall our previous conversation. You are 30 years young, as you so proudly introduced yourself. Shall I pour you a refill on that Earl Grey tea while we continue our conversation?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": query,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response.content}\")\n",
        "\n",
        "  print(f\"Assistant: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SKPNErWSj0b6",
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "source": [
        "### 2.3. Texto din√°mico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho m√°s bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8yNOHL8jrm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello!\n",
            "Good day, sir. How may I assist you today?\n",
            "User: Can you read today's schedule?\n",
            "Good question, sir! Let me just check the daily planner for you. (pause) Ah yes, here is your schedule for today:\n",
            "\n",
            "10:00 AM - Meeting with Mrs. Smith regarding estate matters\n",
            "12:30 PM - Lunch in the dining room\n",
            "2:00 PM - Tennis match at the country club\n",
            "4:00 PM - Meeting with your financial advisor to review quarterly reports\n",
            "\n",
            "Shall I make any adjustments or would you like me to prepare for the meetings, sir?\n",
            "User: exit\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "  query = input(\"User: \")\n",
        "\n",
        "  if query == \"exit\":\n",
        "    break\n",
        "\n",
        "  formated_message = prompt.invoke({\n",
        "      \"user\": \"John\",\n",
        "      \"message\": query,\n",
        "      \"history\": history\n",
        "  })\n",
        "\n",
        "  response = \"\"\n",
        "  # Usamos model.stream en vez de model.invoke\n",
        "  for chunk in model.stream(formated_message):\n",
        "    response += chunk.content\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "  print(\"\")\n",
        "\n",
        "  history.append(f\"User: {query}\")\n",
        "  history.append(f\"Assistant: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvExFNx2lzFE",
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "source": [
        "## 3. Aplicaci√≥n con Gradio\n",
        "\n",
        "Lo que hemos hecho hasta ahora est√° muy bien, pero quiz√° queremos mostrarle nuestro trabajo a nuestros amigos... Gradio ofrece una interfaz muy sencilla para montar una aplicaci√≥n web que sirva para LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDuGfbeHlroE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "ZDuGfbeHlroE",
        "outputId": "b588fa0c-b0ee-44f4-f5fc-9e97fd2b96b2"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "import gradio as gr\n",
        "\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Creamos una plantilla\n",
        "messages = (\"Human\",\n",
        "            \"\"\"\n",
        "            You are a helpful butler named Alfred, helping {user}.\n",
        "            - message history: {history}\n",
        "            - last message: {message}.\n",
        "            \"\"\")\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "def chat(message: str, history: list[str]) -> str:\n",
        "  \"\"\"\n",
        "  Function to chat with the ollama model.\n",
        "  \"\"\"\n",
        "  response = model.invoke(\n",
        "      prompt.invoke({\n",
        "          \"user\": \"John\",\n",
        "          \"message\": message,\n",
        "          \"history\": history\n",
        "      })\n",
        "  )\n",
        "\n",
        "  return response.content\n",
        "\n",
        "demo = gr.ChatInterface(fn=chat)\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8EhpkzLq1G4",
      "metadata": {
        "id": "V8EhpkzLq1G4"
      },
      "source": [
        "## 4. An√°lisis de archivos\n",
        "\n",
        "Una posible aplicaci√≥n interesante de los LLMs es utilizarlos para analizar archivos como PDFs o TXTs. Con gradio es sumamente sencillo integrar esto a la interfaz web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iae3q2pRq0xD",
      "metadata": {
        "id": "iae3q2pRq0xD"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "import gradio as gr\n",
        "\n",
        "# Load model\n",
        "model = init_chat_model(model=\"llama3.1:8b\", model_provider=\"ollama\")\n",
        "\n",
        "# Prompt\n",
        "messages = (\"Human\",\n",
        "    \"\"\"\n",
        "    You are a helpful butler named Alfred, working for {user}.\n",
        "\n",
        "    Documents:\n",
        "    {documents}\n",
        "\n",
        "    Message history:\n",
        "    {history}\n",
        "\n",
        "    Last message:\n",
        "    {message}\n",
        "    \"\"\")\n",
        "\n",
        "prompt = ChatPromptTemplate(messages=messages)\n",
        "\n",
        "# Chat logic\n",
        "def chat_fn(message, history, session_documents):\n",
        "    response = model.invoke(prompt.invoke({\n",
        "        \"user\": \"John\",\n",
        "        \"history\": history,\n",
        "        \"message\": message,\n",
        "        \"documents\": session_documents\n",
        "    }))\n",
        "    return response.content\n",
        "\n",
        "# File loader\n",
        "def load_document(file, session_documents):\n",
        "    print(\"Loading:\", file.name)\n",
        "    if file.name.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file.name)\n",
        "    elif file.name.endswith(\".txt\"):\n",
        "        loader = TextLoader(file.name)\n",
        "    else:\n",
        "        return \"Unsupported file type\", session_documents\n",
        "\n",
        "    docs = loader.load()\n",
        "    texts = [doc.page_content for doc in docs]\n",
        "    updated_docs = session_documents + texts\n",
        "\n",
        "    print(\"Total documents now:\", len(updated_docs))\n",
        "    return \"Document loaded\", updated_docs\n",
        "\n",
        "# Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Alfred, Your Document Butler\")\n",
        "\n",
        "    # Per-session doc memory\n",
        "    session_documents = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(file_types=[\".pdf\", \".txt\"], label=\"Upload a document\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    file_input.change(fn=load_document, inputs=[file_input, session_documents], outputs=[status, session_documents])\n",
        "\n",
        "    chatbot = gr.ChatInterface(\n",
        "        fn=chat_fn,\n",
        "        additional_inputs=[session_documents]\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0119a25",
      "metadata": {},
      "source": [
        "## 5. Conclusiones\n",
        "\n",
        "Este taller ha demostrado que es totalmente posible correr modelos de lenguaje grandes (LLMs) **de forma local**, sin depender de servicios externos en la nube. Gracias a herramientas como **Ollama**, **Langchain** y **Gradio**, puedes construir r√°pidamente aplicaciones conversacionales personalizadas, privadas y adaptables.\n",
        "\n",
        "Al finalizar este notebook, deber√≠as tener un conocimiento b√°sico sobre:\n",
        "\n",
        "- C√≥mo instalar y correr LLMs localmente usando Ollama.\n",
        "- C√≥mo crear interacciones personalizadas y memorias conversacionales con Langchain.\n",
        "- C√≥mo construir una interfaz de usuario sencilla con Gradio.\n",
        "- C√≥mo extender la funcionalidad del modelo para trabajar con archivos (como PDFs).\n",
        "\n",
        "Este es solo el comienzo: pueden adaptar esta arquitectura para hacer agentes inteligentes (ver [Curso de agentes](https://huggingface.co/learn/agents-course/en/unit0/introduction)), asistentes para tu trabajo, chatbots educativos, o incluso sistemas RAG (ver [Taller de RAG con uv](https://github.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session.git) y [Taller de RAG con poetry](https://github.com/Antonio-Tresol/ai_workshop_2025_gen_ai_session.git)).\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por participar en el taller üôå ¬°A seguir creando con IA generativa!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
