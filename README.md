# ai_workshop_2025_gen_ai_session_2

# Taller: Construyendo una App con Gen AI (LLMs locales)  

Este taller tiene como objetivo ense침ar c칩mo utilizar modelos de lenguaje de gran tama침o (LLMs) de manera local utilizando [Ollama](https://ollama.com/), e integrarlos con herramientas como [Langchain](https://www.langchain.com/) y [Gradio](https://www.gradio.app/) para construir una aplicaci칩n conversacional funcional.

---

## 游댢 쯈u칠 se hace en este taller?

1. **Instalaci칩n de dependencias**  
   Se instala y configura **Ollama** para correr modelos localmente en la m치quina de Colab. Tambi칠n se instalan bibliotecas clave como `langchain`, `gradio`, `ollama` y `pypdf`.

2. **Uso de LLMs locales con Ollama**  
   Se configura un servidor local usando `ollama serve`, se descarga un modelo (por ejemplo, `llama3.1:8b`) y se prueba la generaci칩n de texto.

3. **Interacci칩n avanzada con Langchain**  
   Se introduce c칩mo Langchain ayuda a estructurar mensajes con plantillas y a mantener el historial de conversaciones (memoria conversacional). Tambi칠n se trabaja con generaci칩n din치mica de texto token a token.

4. **Creaci칩n de una aplicaci칩n con Gradio**  
   Se implementa una interfaz web para conversar con el modelo desde el navegador.

5. **An치lisis de archivos**  
   Se extiende la funcionalidad para permitir cargar archivos PDF o TXT, analizarlos y obtener respuestas del modelo sobre su contenido.

---

## 游닄 Bibliotecas utilizadas

- [`ollama`](https://pypi.org/project/ollama/): Ejecutar y gestionar LLMs de forma local.
- [`langchain`](https://www.langchain.com/): Orquestar interacciones con modelos de lenguaje.
- [`gradio`](https://www.gradio.app/): Crear interfaces web para modelos de ML.
- `langchain_community`: Complementos para proveedores no incluidos en el paquete base.
- `pypdf`: Lectura de archivos PDF en Python.

---

## 游 C칩mo ejecutarlo

Este proyecto est치 pensado para correr directamente en **Google Colab**. Para comenzar:

1. Abre el notebook en Colab desde este enlace:  
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/incomplete.ipynb)

2. Aseg칰rate de seleccionar un entorno con GPU (`Runtime > Change runtime type > GPU`).

3. Ejecuta cada celda paso a paso siguiendo las instrucciones del notebook.

---

## 游 Requisitos

- Cuenta de Google para usar Colab.
- Acceso a Internet para descargar modelos y bibliotecas.
- Familiaridad b치sica con Python (recomendado).

---

## 游눠 Notas

- Ollama no est치 dise침ado para funcionar en entornos en la nube como Colab de forma nativa, pero este taller muestra c칩mo emular su ejecuci칩n.
- El modelo descargado puede tardar un poco seg칰n su tama침o.
- Gradio puede no funcionar con su interfaz visual completa dentro de Colab; se recomienda probar localmente para una experiencia m치s fluida.

---

## 游닓 Contacto

Para dudas o comentarios, contactar a [@EnriqueVilchezL](https://github.com/EnriqueVilchezL)