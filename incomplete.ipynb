{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ‚ú®\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/incomplete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una peque√±a aplicaci√≥n encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6aBAC3pYLOK",
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "source": [
        "## 1. Instalar dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKIhyOf8XSUO",
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la m√°quina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de c√≥digo abierto que permite ejecutar modelos de lenguaje de gran tama√±o (LLM) directamente en sus computadoras, sin necesidad de conexi√≥n a Internet ni servicios en la nube. Esto la convierte en una opci√≥n ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para m√°s informaci√≥n pueden visitar [Ollama](https://ollama.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cOOT3FxzYARq",
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bdd9b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bdd9b1",
        "outputId": "028b917e-ac0b-4f7f-885a-299092ca72dc",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UuWq73LFYIoa",
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicaci√≥n en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para m√°s informaci√≥n pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para m√°s informaci√≥n pueden visitar [Gradio](https://www.gradio.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d87df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d87df4",
        "outputId": "7073f8d6-141b-4892-8b21-f17410807b1e",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain_community gradio ollama pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la m√°quina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzYYFcBVZ_A3",
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la m√°quina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cYzgfe_HaOpp",
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Py2n2IDaa_R1",
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la m√°quina local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ktxYocQ0bHvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxYocQ0bHvD",
        "outputId": "0ebe8511-90ae-4724-82a3-30ce2cea8751"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eWRyDY0bWV7",
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZ7rRKmZbVsA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HblGBKi-cg3O",
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podr√≠amos usar la funci√≥n \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar m√°s el modelo y el proovedor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R2KT4RIrccFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "J5YT2nFAeHJp",
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes ‚Äútal cual‚Äù se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Qui√©n est√° hablando (user, assistant, system)\n",
        "- Cu√°l es el contexto\n",
        "- D√≥nde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas var√≠an seg√∫n el modelo y pueden incluir delimitadores especiales, instrucciones o formato espec√≠fico. Generalmente se introducen tokens especiales antes de cada interacci√≥n. Esto le permite al modelo entender el contexto en el que interact√∫a.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA √∫til.\n",
        "<|user|>\n",
        "¬øCu√°l es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJLg-pNVfV5Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Qo8y0RgOgbV9",
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione expl√≠citamente el historial.\n",
        "- Cada interacci√≥n con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la ‚Äúmemoria‚Äù del modelo est√° limitada al contenido del mensaje que se le env√≠a en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversaci√≥n con un modelo como LLaMA, GPT o Mistral, el modelo no guarda informaci√≥n sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurri√≥ en la ronda 1, solo podr√° responder correctamente si se le incluye el historial de forma expl√≠cita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "On_wIWx3idxC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "V5n3zVdRi4b8",
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "source": [
        "Para dar la ilusi√≥n de memoria en aplicaciones pr√°cticas, se pasa todo el historial de conversaci√≥n como parte del nuevo mensaje (prompt chaining)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LT4hPK7hjDX6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "SKPNErWSj0b6",
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "source": [
        "### 2.3. Texto din√°mico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho m√°s bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8yNOHL8jrm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "pvExFNx2lzFE",
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "source": [
        "## 3. Aplicaci√≥n con Gradio\n",
        "\n",
        "Lo que hemos hecho hasta ahora est√° muy bien, pero quiz√° queremos mostrarle nuestro trabajo a nuestros amigos... Gradio ofrece una interfaz muy sencilla para montar una aplicaci√≥n web que sirva para LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDuGfbeHlroE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "ZDuGfbeHlroE",
        "outputId": "b588fa0c-b0ee-44f4-f5fc-9e97fd2b96b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "V8EhpkzLq1G4",
      "metadata": {
        "id": "V8EhpkzLq1G4"
      },
      "source": [
        "## 4. An√°lisis de archivos\n",
        "\n",
        "Una posible aplicaci√≥n interesante de los LLMs es utilizarlos para analizar archivos como PDFs o TXTs. Con gradio es sumamente sencillo integrar esto a la interfaz web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iae3q2pRq0xD",
      "metadata": {
        "id": "iae3q2pRq0xD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5ca1c3a0",
      "metadata": {},
      "source": [
        "## 5. Conclusiones\n",
        "\n",
        "Este taller ha demostrado que es totalmente posible correr modelos de lenguaje grandes (LLMs) **de forma local**, sin depender de servicios externos en la nube. Gracias a herramientas como **Ollama**, **Langchain** y **Gradio**, puedes construir r√°pidamente aplicaciones conversacionales personalizadas, privadas y adaptables.\n",
        "\n",
        "Al finalizar este notebook, deber√≠as tener un conocimiento b√°sico sobre:\n",
        "\n",
        "- C√≥mo instalar y correr LLMs localmente usando Ollama.\n",
        "- C√≥mo crear interacciones personalizadas y memorias conversacionales con Langchain.\n",
        "- C√≥mo construir una interfaz de usuario sencilla con Gradio.\n",
        "- C√≥mo extender la funcionalidad del modelo para trabajar con archivos (como PDFs).\n",
        "\n",
        "Este es solo el comienzo: pueden adaptar esta arquitectura para hacer agentes inteligentes (ver [Curso de agentes](https://huggingface.co/learn/agents-course/en/unit0/introduction)), asistentes para tu trabajo, chatbots educativos, o incluso sistemas RAG (ver [Taller de RAG con uv](https://github.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session.git) y [Taller de RAG con poetry](https://github.com/Antonio-Tresol/ai_workshop_2025_gen_ai_session.git)).\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por participar en el taller üôå ¬°A seguir creando con IA generativa!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
