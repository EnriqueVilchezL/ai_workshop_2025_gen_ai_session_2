{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93e46bab",
      "metadata": {
        "id": "93e46bab"
      },
      "source": [
        "# Construyamos un App con Gen AI ✨\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session_2/blob/main/incomplete.ipynb)\n",
        "\n",
        "En este taller vamos a aprender a utilizar grandes modelos de lenguaje de manera local, y a construir una pequeña aplicación encima de ese servicio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R6aBAC3pYLOK",
      "metadata": {
        "id": "R6aBAC3pYLOK"
      },
      "source": [
        "## 1. Instalar dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKIhyOf8XSUO",
      "metadata": {
        "id": "xKIhyOf8XSUO"
      },
      "source": [
        "### 1.1. Ollama\n",
        "\n",
        "Primero, vamos a instalar **Ollama** en la máquina que nos ofrece Google Colab.\n",
        "\n",
        "**Ollama** es una herramienta de código abierto que permite ejecutar modelos de lenguaje de gran tamaño (LLM) directamente en sus computadoras, sin necesidad de conexión a Internet ni servicios en la nube. Esto la convierte en una opción ideal para desarrolladores, investigadores y empresas que buscan mantener el control total sobre sus datos y reducir costos asociados a APIs externas.\n",
        "\n",
        "Para más información pueden visitar [Ollama](https://ollama.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cOOT3FxzYARq",
      "metadata": {
        "id": "cOOT3FxzYARq"
      },
      "source": [
        "En un sistema Linux, Ollama se puede instalar con:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bdd9b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bdd9b1",
        "outputId": "028b917e-ac0b-4f7f-885a-299092ca72dc",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!sudo apt -q update\n",
        "!sudo apt -q install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UuWq73LFYIoa",
      "metadata": {
        "id": "UuWq73LFYIoa"
      },
      "source": [
        "### 1.2. Python\n",
        "\n",
        "Ahora, vamos a instalar los paquetes necesarios para ejecutar nuestra aplicación en python. Las principales bibliotecas que vamos a usar son:\n",
        "\n",
        "1. Langchain: Permite usar LLMs de manera sencilla, ya sea de servicios en la nube o local. Para más información pueden visitar [Langchain](https://www.langchain.com/).\n",
        "\n",
        "2. Gradio: Permite montar interfaces web para modelos de inteligencia artificial de forma sencilla. Para más información pueden visitar [Gradio](https://www.gradio.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d87df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08d87df4",
        "outputId": "7073f8d6-141b-4892-8b21-f17410807b1e",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain_community gradio ollama pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87c4c01",
      "metadata": {
        "id": "b87c4c01"
      },
      "source": [
        " ## 2. Iniciando con LLMs locales\n",
        "\n",
        " Ahora, vamos a utilizar LLMs de forma local, descargando modelos en la máquina de Google Colab y aprovechando la GPU gratuita que nos ofrece."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzYYFcBVZ_A3",
      "metadata": {
        "id": "HzYYFcBVZ_A3"
      },
      "source": [
        "### 2.1. Correr ollama\n",
        "\n",
        "Primero, vamos a correr ollama. Ollama funciona como un servidor que escucha en un puerto de su computadora local, en el cual va a ejecutar LLMs. Normalmente, ollama se corre desde una terminal con el comando \"ollama serve\", pero como no tenemos acceso a una terminal para la máquina de Google Colab, vamos a utilizar la biblioteca de [Ollama en python](https://github.com/ollama/ollama-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cYzgfe_HaOpp",
      "metadata": {
        "id": "cYzgfe_HaOpp"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_server() -> None:\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "threading.Thread(target=run_ollama_server).start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Py2n2IDaa_R1",
      "metadata": {
        "id": "Py2n2IDaa_R1"
      },
      "source": [
        "Ahora, vamos a descargar un modelo de la nube a la máquina local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ktxYocQ0bHvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxYocQ0bHvD",
        "outputId": "0ebe8511-90ae-4724-82a3-30ce2cea8751"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "ollama.pull(\"llama3.1:8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eWRyDY0bWV7",
      "metadata": {
        "id": "1eWRyDY0bWV7"
      },
      "source": [
        "Podemos probar el modelo que acabamos de descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZ7rRKmZbVsA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ7rRKmZbVsA",
        "outputId": "7b9c2f32-f1a9-4ab5-dab2-dc8dd5f5acbe"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from ollama import AsyncClient\n",
        "\n",
        "async def chat(message: str):\n",
        "    message = {'role': 'user', 'content': message}\n",
        "    response = await AsyncClient().chat(model='llama3.1:8b', messages=[message])\n",
        "    print(response.message.content)\n",
        "\n",
        "await chat('Why is the sky blue?')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HblGBKi-cg3O",
      "metadata": {
        "id": "HblGBKi-cg3O"
      },
      "source": [
        "### 2.2. Hacer un chat con langchain y ollama\n",
        "\n",
        "Podríamos usar la función \"chat\" de ollama para interactuar con el modelo, sin embargo, langchain da muchas facilidades para controlar más el modelo y el proovedor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R2KT4RIrccFD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KT4RIrccFD",
        "outputId": "321d1747-a47c-4293-8f85-161e681752ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "J5YT2nFAeHJp",
      "metadata": {
        "id": "J5YT2nFAeHJp"
      },
      "source": [
        "Esto funciona. Sin embargo, hay dos particularidades de los LLMs que vale la pena conocer:\n",
        "\n",
        "#### **Plantillas**\n",
        "\n",
        "Los LLMs no interpretan directamente los mensajes “tal cual” se ingresan. Internamente, los mensajes se organizan mediante plantillas de texto estructuradas, que ayudan al modelo a comprender:\n",
        "- Quién está hablando (user, assistant, system)\n",
        "- Cuál es el contexto\n",
        "- Dónde debe empezar a generar su respuesta\n",
        "\n",
        "Estas plantillas varían según el modelo y pueden incluir delimitadores especiales, instrucciones o formato específico. Generalmente se introducen tokens especiales antes de cada interacción. Esto le permite al modelo entender el contexto en el que interactúa.\n",
        "\n",
        "Un ejemplo de esto es:\n",
        "```\n",
        "<|system|>\n",
        "Eres una IA útil.\n",
        "<|user|>\n",
        "¿Cuál es la capital de Francia?\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Afortunadamente, Langchain puede manejar estos detalles por nosotros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJLg-pNVfV5Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLg-pNVfV5Y",
        "outputId": "33dd3d37-27e7-42c2-9499-561bddd8eed0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Qo8y0RgOgbV9",
      "metadata": {
        "id": "Qo8y0RgOgbV9"
      },
      "source": [
        "#### **Memoria**\n",
        "\n",
        "Es importante entender que, a pesar de parecer inteligentes y conversacionales, los modelos de lenguaje no tienen memoria a largo plazo. Esto significa que:\n",
        "- No recuerdan conversaciones anteriores a menos que se les proporcione explícitamente el historial.\n",
        "- Cada interacción con el modelo es independiente, a menos que se incluya el contexto anterior en el mismo prompt.\n",
        "- Toda la “memoria” del modelo está limitada al contenido del mensaje que se le envía en ese momento.\n",
        "\n",
        "Cuando se mantiene una conversación con un modelo como LLaMA, GPT o Mistral, el modelo no guarda información sobre lo que se dijo antes. Si se le pregunta algo en la ronda 2 que depende de lo que ocurrió en la ronda 1, solo podrá responder correctamente si se le incluye el historial de forma explícita."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "On_wIWx3idxC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On_wIWx3idxC",
        "outputId": "06b21125-a6a2-4451-cf18-8bfe1e556056"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "V5n3zVdRi4b8",
      "metadata": {
        "id": "V5n3zVdRi4b8"
      },
      "source": [
        "Para dar la ilusión de memoria en aplicaciones prácticas, se pasa todo el historial de conversación como parte del nuevo mensaje (prompt chaining)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LT4hPK7hjDX6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT4hPK7hjDX6",
        "outputId": "6019ce60-90a5-4069-8bb1-e32e931ad0cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "SKPNErWSj0b6",
      "metadata": {
        "id": "SKPNErWSj0b6"
      },
      "source": [
        "### 2.3. Texto dinámico\n",
        "\n",
        "Si corremos ollama en una terminal, vamos a ver que es mucho más bonito que el chat que hemos montado hasta el momento. Esto se debe a que los LLMs producen token por token, y no oraciones completas cada vez que mandamos una entrada, por lo que se puede imprimir los tokens conforme los va generando.\n",
        "\n",
        "Langchain ofrece una interfaz sencilla para hacer esto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8yNOHL8jrm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8yNOHL8jrm9",
        "outputId": "5032cdd4-326e-44d6-ede9-60dd41cc11dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "pvExFNx2lzFE",
      "metadata": {
        "id": "pvExFNx2lzFE"
      },
      "source": [
        "## 3. Aplicación con Gradio\n",
        "\n",
        "Lo que hemos hecho hasta ahora está muy bien, pero quizá queremos mostrarle nuestro trabajo a nuestros amigos... Gradio ofrece una interfaz muy sencilla para montar una aplicación web que sirva para LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZDuGfbeHlroE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "ZDuGfbeHlroE",
        "outputId": "b588fa0c-b0ee-44f4-f5fc-9e97fd2b96b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "V8EhpkzLq1G4",
      "metadata": {
        "id": "V8EhpkzLq1G4"
      },
      "source": [
        "## 4. Análisis de archivos\n",
        "\n",
        "Una posible aplicación interesante de los LLMs es utilizarlos para analizar archivos como PDFs o TXTs. Con gradio es sumamente sencillo integrar esto a la interfaz web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iae3q2pRq0xD",
      "metadata": {
        "id": "iae3q2pRq0xD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5ca1c3a0",
      "metadata": {},
      "source": [
        "## 5. Conclusiones\n",
        "\n",
        "Este taller ha demostrado que es totalmente posible correr modelos de lenguaje grandes (LLMs) **de forma local**, sin depender de servicios externos en la nube. Gracias a herramientas como **Ollama**, **Langchain** y **Gradio**, puedes construir rápidamente aplicaciones conversacionales personalizadas, privadas y adaptables.\n",
        "\n",
        "Al finalizar este notebook, deberías tener un conocimiento básico sobre:\n",
        "\n",
        "- Cómo instalar y correr LLMs localmente usando Ollama.\n",
        "- Cómo crear interacciones personalizadas y memorias conversacionales con Langchain.\n",
        "- Cómo construir una interfaz de usuario sencilla con Gradio.\n",
        "- Cómo extender la funcionalidad del modelo para trabajar con archivos (como PDFs).\n",
        "\n",
        "Este es solo el comienzo: pueden adaptar esta arquitectura para hacer agentes inteligentes (ver [Curso de agentes](https://huggingface.co/learn/agents-course/en/unit0/introduction)), asistentes para tu trabajo, chatbots educativos, o incluso sistemas RAG (ver [Taller de RAG con uv](https://github.com/EnriqueVilchezL/ai_workshop_2025_gen_ai_session.git) y [Taller de RAG con poetry](https://github.com/Antonio-Tresol/ai_workshop_2025_gen_ai_session.git)).\n",
        "\n",
        "---\n",
        "\n",
        "Gracias por participar en el taller 🙌 ¡A seguir creando con IA generativa!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
